{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Phase - Logging Setup\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "LOG_FILE = \"dataset-exploration.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    filename=LOG_FILE,\n",
    "    filemode=\"w\",  # Overwrite previous logs on each run\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Redirect print statements to the logger\n",
    "class LoggerWriter:\n",
    "    def __init__(self, level):\n",
    "        self.level = level\n",
    "    def write(self, message):\n",
    "        if message.strip():\n",
    "            self.level(message.strip())\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "sys.stdout = LoggerWriter(logger.info)\n",
    "sys.stderr = LoggerWriter(logger.error)\n",
    "\n",
    "print(\"=== Exploration Phase Notebook: Logging Setup Complete ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Phase - Cell 1: Load Prepared Datasets\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path where the prepared datasets are stored\n",
    "PREPARED_PATH = \"./data/prepared/\"\n",
    "\n",
    "# Dictionary mapping dataset names to their prepared file names\n",
    "files = {\n",
    "    \"pbj_nurse\": \"pbj_nurse_prepared.parquet\",\n",
    "    \"pbj_non_nurse\": \"pbj_non_nurse_prepared.parquet\",\n",
    "    \"qrp_provider\": \"qrp_provider_prepared.parquet\",\n",
    "    \"nh_survey\": \"nh_survey_prepared.parquet\",\n",
    "    \"nh_quality_mds\": \"nh_quality_mds_prepared.parquet\",\n",
    "    \"nh_ownership\": \"nh_ownership_prepared.parquet\",\n",
    "    \"nh_citations\": \"nh_citations_prepared.parquet\"\n",
    "}\n",
    "\n",
    "loaded_datasets = {}\n",
    "for key, filename in files.items():\n",
    "    file_path = os.path.join(PREPARED_PATH, filename)\n",
    "    try:\n",
    "        loaded_datasets[key] = pd.read_parquet(file_path)\n",
    "        print(f\"Loaded {key} with shape {loaded_datasets[key].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {key}: {e}\")\n",
    "\n",
    "# Optionally, check the columns of one dataset to verify\n",
    "print(\"Columns in pbj_nurse:\", loaded_datasets['pbj_nurse'].columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Phase - Cell 2: Comprehensive Dataset Logging\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='dataset_exploration.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def log_dataset_info(df, dataset_name):\n",
    "    \"\"\"Logs comprehensive information about a DataFrame.\"\"\"\n",
    "\n",
    "    logging.info(f\"--- Dataset: {dataset_name} ---\")\n",
    "\n",
    "    # Basic Shape\n",
    "    logging.info(f\"Shape: {df.shape}\")\n",
    "\n",
    "    # Columns\n",
    "    logging.info(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Data Types\n",
    "    logging.info(\"Data Types:\\n\" + str(df.dtypes))\n",
    "\n",
    "    # Missing Values\n",
    "    missing_values = df.isnull().sum()\n",
    "    logging.info(\"Missing Values:\\n\" + str(missing_values))\n",
    "    logging.info(f\"Total Missing Values: {missing_values.sum()}\")\n",
    "\n",
    "    # Duplicate Rows\n",
    "    num_duplicates = df.duplicated().sum()\n",
    "    logging.info(f\"Number of Duplicate Rows: {num_duplicates}\")\n",
    "\n",
    "    # Descriptive Statistics (Numeric Columns)\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    if not numeric_cols.empty:\n",
    "        logging.info(\"Descriptive Statistics (Numeric Columns):\\n\" + str(df[numeric_cols].describe()))\n",
    "    else:\n",
    "        logging.info(\"No numeric columns found for descriptive statistics.\")\n",
    "\n",
    "\n",
    "    # Unique Value Counts (for Categorical/Low Cardinality Columns)\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() < 50:  # Adjust threshold as needed\n",
    "            logging.info(f\"Unique Value Counts for '{col}':\\n\" + str(df[col].value_counts()))\n",
    "\n",
    "    # First 5 Rows\n",
    "    logging.info(\"First 5 Rows:\\n\" + str(df.head()))\n",
    "\n",
    "    logging.info(f\"--- End Dataset: {dataset_name} ---\\n\")\n",
    "\n",
    "\n",
    "# Loop through the loaded datasets and log information\n",
    "for dataset_name, df in loaded_datasets.items():\n",
    "    log_dataset_info(df, dataset_name)\n",
    "\n",
    "print(\"Dataset exploration complete.  See 'dataset_exploration.log' for details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipboard_health_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
