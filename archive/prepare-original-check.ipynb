{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Logging\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Define the path to your raw CSV files\n",
    "RAW_DATA_PATH = \"./data/raw/\"\n",
    "LOG_FILE = \"raw_data_verification.log\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    filename=LOG_FILE,\n",
    "    filemode=\"w\",  # Overwrite on each run\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Redirect print statements to the logger\n",
    "class LoggerWriter:\n",
    "    def __init__(self, level):\n",
    "        self.level = level\n",
    "    def write(self, message):\n",
    "        if message.strip():\n",
    "            self.level(message.strip())\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "sys.stdout = LoggerWriter(logger.info)\n",
    "sys.stderr = LoggerWriter(logger.error)\n",
    "\n",
    "print(\"=== Raw Data Verification Notebook: Logging Setup Complete ===\")\n",
    "print(f\"Using RAW_DATA_PATH: {RAW_DATA_PATH}\")\n",
    "print(f\"Logs will be written to: {LOG_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define Dataset Dictionary & Load Raw CSV Files\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== Starting Raw Data Verification ===\")\n",
    "\n",
    "# Dictionary mapping dataset names to the raw CSV filenames\n",
    "raw_datasets = {\n",
    "    \"nh_citations_raw\": \"NH_HealthCitations_Jan2025.csv\",\n",
    "    \"nh_ownership_raw\": \"NH_Ownership_Jan2025.csv\",\n",
    "    \"nh_quality_mds_raw\": \"NH_QualityMsr_MDS_Jan2025.csv\",\n",
    "    \"nh_survey_raw\": \"NH_SurveySummary_Jan2025.csv\",\n",
    "    \"pbj_non_nurse_raw\": \"PBJ_Daily_Non_Nurse_Staffing_Q2_2024.csv\",\n",
    "    \"pbj_nurse_raw\": \"PBJ_Daily_Nurse_Staffing_Q2_2024.csv\",\n",
    "    \"qrp_provider_raw\": \"Skilled_Nursing_Facility_Quality_Reporting_Program_Provider_Data_Jan2025.csv\"\n",
    "}\n",
    "\n",
    "loaded_raw_datasets = {}\n",
    "\n",
    "def load_csv_safely(file_path):\n",
    "    \"\"\"Attempt to load CSV with multiple encodings.\"\"\"\n",
    "    encodings = [\"utf-8\", \"latin1\", \"ISO-8859-1\", \"windows-1252\"]\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=enc, low_memory=False)\n",
    "            print(f\"Loaded {os.path.basename(file_path)} with encoding={enc}, shape={df.shape}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Failed with encoding={enc}: {e}\")\n",
    "    print(f\"All encodings failed for {file_path}\")\n",
    "    return None\n",
    "\n",
    "# Loop through each dataset and load it\n",
    "for key, filename in raw_datasets.items():\n",
    "    file_path = os.path.join(RAW_DATA_PATH, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = load_csv_safely(file_path)\n",
    "        if df is not None:\n",
    "            loaded_raw_datasets[key] = df\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "print(\"=== Summary of Loaded Raw Datasets ===\")\n",
    "if loaded_raw_datasets:\n",
    "    for key, df in loaded_raw_datasets.items():\n",
    "        print(f\"{key}: shape={df.shape}, columns={list(df.columns)}\")\n",
    "else:\n",
    "    print(\"No raw datasets loaded. Check file paths or filenames.\")\n",
    "\n",
    "print(\"=== Raw Data Verification - Load Step Completed ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Basic Checks (Missing, Duplicates, Numeric Stats)\n",
    "\n",
    "if not loaded_raw_datasets:\n",
    "    print(\"No raw datasets to check. Please ensure data is loaded in Cell 2.\")\n",
    "else:\n",
    "    for key, df in loaded_raw_datasets.items():\n",
    "        print(f\"\\n=== Data Quality Checks for {key} ===\")\n",
    "        \n",
    "        # 1. Missing Values\n",
    "        missing_counts = df.isna().sum()\n",
    "        print(\"Missing Value Counts per Column:\")\n",
    "        print(missing_counts)\n",
    "\n",
    "        # 2. Duplicate Rows\n",
    "        duplicates = df.duplicated().sum()\n",
    "        print(f\"Number of Duplicate Rows: {duplicates}\")\n",
    "\n",
    "        # 3. Numeric Stats\n",
    "        numeric_cols = df.select_dtypes(include=[\"int\", \"float\"]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            stats = df[numeric_cols].describe()\n",
    "            print(\"Basic Stats for Numeric Columns:\")\n",
    "            print(stats)\n",
    "        else:\n",
    "            print(\"No numeric columns found for outlier checks.\")\n",
    "\n",
    "print(\"=== Raw Data Verification - Basic Checks Completed ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: (Optional) Compare with Known Preprocessed Shapes\n",
    "\n",
    "# If you want to confirm the raw shapes match your final preprocessed shapes,\n",
    "# you can hard-code the shapes you saw in your logs from the preprocessed data:\n",
    "\n",
    "preprocessed_shapes = {\n",
    "    \"pbj_nurse\": (1325324, 33),\n",
    "    \"pbj_non_nurse\": (1325324, 82),\n",
    "    \"qrp_provider\": (710016, 16),\n",
    "    \"nh_survey\": (44189, 41),\n",
    "    \"nh_quality_mds\": (251464, 23),\n",
    "    \"nh_ownership\": (144651, 13),\n",
    "    \"nh_citations\": (406789, 23)\n",
    "}\n",
    "\n",
    "# Now let's compare them to the raw shapes\n",
    "for key, shape in preprocessed_shapes.items():\n",
    "    # The raw dataset dictionary keys end with '_raw', so let's adapt the name:\n",
    "    raw_key = key + \"_raw\"\n",
    "    if raw_key in loaded_raw_datasets:\n",
    "        raw_df = loaded_raw_datasets[raw_key]\n",
    "        raw_shape = raw_df.shape\n",
    "        print(f\"Comparing {raw_key} to preprocessed '{key}':\")\n",
    "        print(f\"  Raw shape:          {raw_shape}\")\n",
    "        print(f\"  Preprocessed shape: {shape}\")\n",
    "        \n",
    "        # If the shape is the same, or close, that suggests minimal changes.\n",
    "        # If different, that suggests either cleaning, filtering, or combining happened.\n",
    "        if raw_shape == shape:\n",
    "            print(\"  => Shapes match exactly!\")\n",
    "        else:\n",
    "            print(\"  => Shapes differ. This could be normal if you removed duplicates, filtered rows, or changed columns.\")\n",
    "    else:\n",
    "        print(f\"Raw dataset not loaded for {raw_key}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipboard_health_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
